%        File: covo.tex
%     Created: Di Okt 16 10:00  2018 C
% Last Change: Di Okt 16 10:00  2018 C
%

\documentclass[a4paper]{report}
% ***************************PACKAGES***************************
\usepackage{graphicx}
\usepackage{caption,setspace}
\usepackage{subcaption}
\captionsetup[figure]{width=0.7\textwidth,font={small}}
\usepackage{float}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{hyperref}
\usepackage{siunitx}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{mwe}
\usepackage{tabularx}
\usepackage{cite}
\usepackage{hyperref}
\usepackage{bookmark}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\usepackage{array}


% ***************************CONFIGS***************************
% for numbering per section
\numberwithin{figure}{section}
% removing ugly colored rectangles from references
\usepackage{xcolor}
\hypersetup{
    hidelinks,
    colorlinks = false,
    %linkcolor={black},
    %citecolor={black},
    %urlcolor={black}
  }
% command for table tabular alignment
\newcolumntype{L}{>{\raggedright\arraybackslash}X}
% command for argmin and argmax
%\DeclareMathOperator*{\argmax}{arg\,max}
%\DeclareMathOperator*{\argmin}{arg\,min}
\newcommand{\argmin}{\mathop{\mathrm{argmin}}}
\newcommand{\argmax}{\mathop{\mathrm{argmax}}}
\newcommand{\R}{\mathbb{R}}
% commands for cascading subfigures
\newsavebox{\subfloatbox}
\newcommand{\topfloat}[2][\empty]% #1 = caption, #2=image
 {\savebox\subfloatbox{#2}%
  \begin{minipage}[t]{\wd\subfloatbox}
    \usebox\subfloatbox
    \subcaption{#1}
  \end{minipage}}
\newcommand{\bottomfloat}[2][\empty]% #1 = caption, #2=image
 {\savebox\subfloatbox{#2}%
  \begin{minipage}[b]{\wd\subfloatbox}
    \captionsetup{position=top}%
    \subcaption{#1}
    \usebox\subfloatbox
  \end{minipage}}

\algnewcommand\algorithmicforeach{\textbf{for each}}
\algdef{S}[FOR]{ForEach}[1]{\algorithmicforeach\ #1\ \algorithmicdo}

\newcommand*{\vertbar}{\rule[-1ex]{0.5pt}{2.5ex}}
\newcommand*{\horzbar}{\rule[.5ex]{2.5ex}{0.5pt}}


% ***************************ACRONYMS***************************
\usepackage[toc, shortcuts]{glossaries}
\makeglossaries
\newacronym{vo}{VO}{Visual Odometry}



\begin{document}

% ***************************TITLE***************************
\begin{titlepage}
\begin{center}

%  \includegraphics[width=\textwidth]{../fig/tuc_logo.jpg}

\vspace{1.5cm}


{\Huge \textbf{Master Thesis}}\\
\vspace{0.5cm}
{\huge \textbf{An Error Aware RGB-D \\Visual Odometry}}


\vspace{2.5cm}

{\huge \textbf{U\u{g}ur Bolat}}

\vfill

Date:

\vspace{1.2cm}

Supervisors: \\
Dr.-Ing. Sven Lange \\
M.Sc. Tim Pfeifer

\vspace{0.8cm}

Faculty of Electrical Engineering and Information Technology\\
Professorship of Process Automation

\end{center}
\end{titlepage}

% ***************************TABLE OF CONTENT ETC.***************************
\tableofcontents
\newpage
\listoffigures
\listoftables
\newpage
\printglossary[type=\acronymtype,title={Abbreviations}]
\newpage

% ***************************DOCUMENT***************************

\begin{abstract}


\end{abstract}

\newpage

% ***************************CP1-INTRO***************************
\chapter{Introduction} \label{cp_intro}


\newpage

% ***************************CP2-VO***************************

\chapter{Camera Models} \label{cp_cam_models}

RE-WRITE THE FOLLOWING PARAGRAPH - COVER WHAT CAMERA DOES RATHER THAN 
TALKING ABOUT PINHOLE - TALK ABOUT GEOMETRICAL MODELS OF CAMERAS

A camera maps from a 3D world scene to a 2D image plane. We call this process 
projection operation. Since the \acrshort{vo} systems process camera image 
sequences, one has to model this projection operation accurately. One of the 
basic camera modeling technique is the \textit{Pinhole Model} where the projection of 
the 3D points are mapped on a 2D image plane (also called focal plane). 

\section{The Pinhole Model for RGB Camera} \label{sc_pinhole}

Normally, the light rays 
are captured through the camera's lens onto an electronic plate (could be CCD or CMOS) 
that convert light intensity to electrical signals. 
The pinhole model is, on the other hand, an approximation which simplifies 
our calculations.
In this model, the camera centre sits behind the image plane.
The Z-axis, 
so called \textit{principal axis}, of this 
coordinate system points out through the origin of the image plane and the 
point where pierce through image plane is called the \textit{principal point}. 
We can also see how other two axes; i.e., X and Y, are located in Figure-\ref{fig:pinhole} 
and this is known as the \textit{Camera Coordinate System} $(x_{cam}, y_{cam}, z_{cam})$.


\begin{figure}[H]
	\centering
	\includegraphics[width=\linewidth,natwidth=640,natheight=640]
  {fig/ref_imgs/pinhole_model.png}
  \caption{REMEMBER TO DRAW HORIZ/VERT Fx and Fy. The Pinhole Model}
	\label{fig:pinhole}
\end{figure}

Thanks to geometrical propotion property, we can project 
the 3D point $(X, Y, Z)^T$ in Euclidean space $\mathbb{R}^3$
to the 2D point $(U,V)^T = (f_xX/Z, f_yY/Z)^T$ in Euclidean space $\mathbb{R}^2$, 
where $f_x$ and $f_y$ are the \textit{focal lengths} 
between the camera centre and the pricipal 
point with respect to horizontal and vertical axis of the Camera Coordinate 
System respectively.
After projection, we obtain a 2D 
point that we represent on the 
\textit{Image Coordinate Frame} $(u_{img},v_{img})$.

To be more specific,
we can write the projection operation as a linear mapping function 
in the following way if we utilize the homogenous coordinates:

NOTE: NAME x,y,z indices for world and camera coords all below

\begin{equation}
  \begin{pmatrix}
    U\\
    V\\
    1
  \end{pmatrix}
  \sim
  Z
  \begin{pmatrix}
    f_xX/Z\\
    f_yY/Z\\
    1
  \end{pmatrix}
  =
  \begin{pmatrix}
    f_xX\\
    f_yY\\
    Z
  \end{pmatrix}
  =
  \begin{bmatrix}
    f_x & 0 & 0 & 0\\
    0 & f_y & 0 & 0\\
    0 & 0 & 1 & 0\\
  \end{bmatrix}
  \begin{pmatrix}
    X\\
    Y\\
    Z\\
    1
  \end{pmatrix}
\end{equation} \label{eq:proj_func_w_f}

This equation applies for the case when 3D points are 
projected onto a plane where the principal point is the origin. 
However, the common convention in partice 
is to have the origin at the (not entirely sure) left-bottom corner not in the centre.

\begin{figure}[H]
	\centering
	\includegraphics[width=\linewidth,natwidth=640,natheight=640]
  {fig/ref_imgs/pinhole_offset.png}
	\caption{Principle Point Offset}
	\label{fig:pinhole_offset}
\end{figure}

Thus, we get offsets, which can further be added into our function:

\begin{equation}
  \begin{pmatrix}
    U\\
    V\\
    1
  \end{pmatrix}
  \sim
  Z
  \begin{pmatrix}
    (f_xX + Z c_x)/Z\\
    (f_yY + Z c_y)/Z\\
    1
  \end{pmatrix}
  =
  \begin{pmatrix}
    f_xX + Z c_x\\
    f_yY + Z c_y\\
    Z
  \end{pmatrix}
  =
  \begin{bmatrix}
    f_x & 0 & c_x & 0\\
    0 & f_y & c_y & 0\\
    0 & 0 & 1 & 0\\
  \end{bmatrix}
  \begin{pmatrix}
    X\\
    Y\\
    Z\\
    1
  \end{pmatrix}
\end{equation} \label{eq:proj_func_w_f_c}

where $c_x$ and $c_y$ are coordinates of the principal point \textbf{p}.

In addition to pricipal offsets, inaccurately synchronized pixel-sampling 
process can result in \textit{skewed pixels}. This camera imperfection leads to 
non-square pixels as seen in Figure-\ref{fig:skewed}.

\begin{figure}[H]
	\centering
  \includegraphics[width=0.5\linewidth,natwidth=640,natheight=640]
  {fig/ref_imgs/skew.png}
  \caption{REMEMBER TO DRAW FOR X and Y.Skew and Non-Square Pixels}
	\label{fig:skewed}
\end{figure}

We can scale the square pixels, having 1:1 pixel aspect ratio, with the 
corresponding skew parameters $\eta_x$, $\eta_y$ and $s$:

\begin{equation}
  \begin{pmatrix}
    U\\
    V\\
    1
  \end{pmatrix}
  \sim
  \begin{bmatrix}
    f_x\eta_x & s & c_x & 0\\
    0 & f_y\eta_y & c_y & 0\\
    0 & 0 & 1 & 0\\
  \end{bmatrix}
  \begin{pmatrix}
    X\\
    Y\\
    Z\\
    1
  \end{pmatrix}
  =
  \begin{bmatrix}
    \alpha_x & s & c_x & 0\\
    0 & \alpha_y & c_y & 0\\
    0 & 0 & 1 & 0\\
  \end{bmatrix}
  \begin{pmatrix}
    X\\
    Y\\
    Z\\
    1
  \end{pmatrix}
\end{equation} \label{eq:proj_func_w_square_pix_skew}

Generally, the skewed pixels issues occurred in ealier versions of CCD cameras 
and this is mostly fixed in new generation digital cameras. Therefore, 
we can neglate this effect by taking $\eta_x=1$, $\eta_y=1$ and $s=0$.

Next, we can extract:

\begin{equation}
  \mathbf{K} = 
  \begin{bmatrix}
    \alpha_x & s & c_x\\
    0 & \alpha_y & c_y\\
    0 & 0 & 1\\
  \end{bmatrix}
\end{equation} \label{eq:k_matrix}

The $\mathbf{K}$ matrix is called 
\textit{intrinsic parameters matrix}, which represents the characteristics of 
a camera sensor. Note that, we 
can further reformulate the notation \ref{eq:proj_func_w_square_pix_skew} in more 
compact form:

\begin{equation}
  \mathbf{x_{img}} = \mathbf{K}[\mathbf{I}|\mathbf{0}]\mathbf{X_{cam}}
\end{equation} \label{eq:simplyfied_proj_func}

\begin{figure}[H]
	\centering
  \includegraphics[width=\linewidth,natwidth=640,natheight=640]
  {fig/ref_imgs/cam_model_rot_trans.png}
  \caption{Camera Rotation and Translation}
  \label{fig:cam_model_rot_trans}
\end{figure}

Remember that, the Z-axis of the Camera Coordinate System aligns with the 
principal axis that is \textit{local} to camera frame. In fact, we have 3D points that 
we represent in the \textit{World Coordinate System} 
which we refer as the \textit{global} frame. 
These two coordinates systems 
can be transformed one another by a rotation and a translation as it is 
depicted in Figure-\ref{fig:cam_model_rot_trans} but we are interested in 
converting from the World Coordinate to Camera Coordinate System in this case.
To do so, we first perform series of rotations around each axis of the 
cartesian coordinate system in Euclidean space by using 
\textit{rotation matrices} where $R_x, R_y, R_z \in SO(3)$ is the rotation group:

\begin{equation}
  R_x(\theta) = 
  \begin{bmatrix}
    1 & 0 & 0\\
    0 & cos\theta & -sin\theta\\
    0 & sin\theta & cos\theta
  \end{bmatrix}
\end{equation} \label{eq:rot_matrx_x}

\begin{equation}
  R_y(\theta) = 
  \begin{bmatrix}
    cos\theta & 0 & -sin\theta\\
    0 & 1 & 0\\
    sin\theta & 0 & cos\theta
  \end{bmatrix}
\end{equation} \label{eq:rot_matrx_y}

\begin{equation}
  R_z(\theta) = 
  \begin{bmatrix}
    cos\theta & -sin\theta & 0\\
    sin\theta & cos\theta & 0\\
    0 & 0 & 1
  \end{bmatrix}
\end{equation} \label{eq:rot_matrx_z}

One can concatenate all three rotations about axes z, y, x respectively:

\begin{equation}
  \mathbf{R} = \mathbf{R_z(\gamma)}\mathbf{R_y(\beta)}\mathbf{R_x(\alpha)}
  =
  \begin{bmatrix}
    r_{11} & r_{12} & r_{13}\\
    r_{21} & r_{22} & r_{23}\\
    r_{31} & r_{32} & r_{33}\\
  \end{bmatrix}
\end{equation} \label{eq:rot_matrix_derivation}

Then, perform a translation $\mathbf{t} \in \R^{3x1}$:

\begin{equation}
  \mathbf{t} = 
  \begin{bmatrix}
    t_x \\ t_y \\ t_z
  \end{bmatrix}
\end{equation} \label{eq:translation}

We can also compound the rotation matrix and the translation vector into 
one matrix:

\begin{equation}
  \mathbf{T} =
  \begin{bmatrix}
    r_{11} & r_{12} & r_{13} & t_x\\
    r_{21} & r_{22} & r_{23} & t_y\\
    r_{31} & r_{32} & r_{33} & t_z\\
  \end{bmatrix}
\end{equation} \label{eq:transformation_matrix}

The $\mathbf{T} \in \R^{4x3}$ matrix in fact represents 
a \textit{rigid-body transformation}, which we call 
\textit{extrinsic camera parameters}.

Finally, we combine intrinsic $\mathbf{K}$ and 
extrinsic $\mathbf{T}$ matrices to form the following notation: 

\begin{equation}
  \mathbf{x_{img}} = 
  \mathbf{P}\mathbf{X_{world}} = 
  \mathbf{K}\mathbf{T}\mathbf{X_{world}} = 
  \mathbf{K}[\mathbf{R}|\mathbf{t}]\mathbf{X_{world}} =
  \mathbf{K}\mathbf{X_{cam}}
\end{equation} \label{eq:simplyfied_proj_func_1}

\begin{equation}
  \mathbf{x_{img}} = \mathbf{F_{proj}}(\mathbf{X_{world}})
\end{equation} \label{eq:simplyfied_proj_func_2}

where $\mathbf{F_{proj}}(\mathbf{X_{world}})$ is the 
\textit{projective transformation function}, which takes 
the 3D points in the World Coordinate System, transforms to 
the Camera Coordinate Systems and then maps them into the Image 
Coordinate Systems.

To build any reliable computer vision application with digital cameras, it is 
to important to find a good $\mathbf{P}$ \textit{projection matrix}. 
The next section describes one of many numerical methods for estimationg this 
matrix in literature.

\section{The Triangulation Model for Depth Camera} \label{sc_depth_model}

Projected images captured by RGB cameras lack depth and angle 
information. To acquire these information, two main techniques are developed; 
e.g., passive stereo cameras and active stereo cameras. For passive stereo cameras, 
typically two synchronized cameras are placed horizontally with a known distance to each other. 
Hence, one usually exploits the triangulation technique to calculate the depth. 
Whereas, for active stereo camera, one typically has one light projector and 
one camera sensor. For example, in Kinect, a Infrared (IR) laser projects 
structured IR speckle light pattern on a object, and then 
the deformed light due to 3D geotmerty of the object are 
captured with a monochrome IR camera from different 
position (see figure-\ref{fig:kinect_depth_img}). 

\begin{figure}[H]
	\centering
  \includegraphics[width=0.7\linewidth,natwidth=640,natheight=640]
  {fig/ref_imgs/kinect_depth_img.png}
  \caption{Kinect Depth Image Construction}
	\label{fig:kinect_depth_img}
\end{figure}

Since we use Kinect in order to retrive depth information for our experiments, 
we will be modeling active stereo vision principle even though the 
basic principle behind them is the same mathematical model, which is 
the \textit{triangulation model}. This model is another geometrical model that 
takes advantages of similarity triangles to calculate the depth. 

\begin{figure}[H]
	\centering
  \includegraphics[width=0.7\linewidth,natwidth=640,natheight=640]
  {fig/ref_imgs/kinect_depth_measurement.png}
  \caption{Kinect's Depth Measurement}
	\label{fig:uncertainty_matching}
\end{figure}

In this setup, 
we again utilize the Camera Coordinate system similar to RGB camera
and IR camera with $f$ focal length is placed by facing perpendicular to 
the (principle axis) Z-axis at the origin. Then, IR laser projector is 
also placed along the X-axis parallel to the IR camera with $b$ distance. 
Additionally, we measure $d$ as a \textit{disparity} data and 
the maximum range that can be measured refers to $Z_r$. Ultimately, we are 
interested in finding $Z_i$ distance if depth information of point $k$ 
is desired. For doing so, we build two useful relationships using similarity of triangles:


\begin{equation}
  \frac{D}{b} = \frac{Z_r - Z_i}{Z_r} \text{ and } \frac{d}{f} = \frac{D}{Z_i}
\end{equation}

If the depth camera paramters such as $(f, b, Z_r)$ is 
calibrated and we measure $d$ disparity data, 
we can easily extract $Z_i$ depth information with the following formula:

\begin{equation}
  Z_i = \frac{Z_r}{1+\frac{Z_r}{fb}d}
\end{equation}\label{eq:depth_wo_disparity}

Another important point to note is that Kinect or other depth cameras in pratice might 
not provide the depth metric information directly. For instance, 
Kinect provides us disparity image data that correspond to inverse depth 
quantized with 11 bits and the relationship between disparity data and 
real depth is non-linear as shown in \ref{fig:depth_disparity_relation}.

\begin{figure}[H]
	\centering
  \includegraphics[width=0.7\linewidth,natwidth=640,natheight=640]
  {fig/ref_imgs/depth_disparity_relation.png}
  \caption{Inverse Depth and Disparity Data Relationship}
	\label{fig:depth_disparity_relation}
\end{figure}

Thus, we need to update depth equation \ref{eq:depth_wo_disparity} by 
taking its inverse and 
introducing denormalization factor replacing with $d$ with $md'+n$:

\begin{equation}
  Z_i^{-1} = (\frac{m}{fb})d' + (Z_r^{-1} + \frac{n}{fb})
\end{equation}

To make it convenient to calculate, we can take its inverse again:

\begin{equation}
  Z_i = \frac{1}{(\frac{m}{fb})d' + (Z_r^{-1} + \frac{n}{fb})}
\end{equation}\label{eq:depth_w_disparity}

Now that we know how to get metric depth information $Z_i$ from disparity data by 
utilizing the triangulation model, we can discuss 
how
$X_i$ and $Y_i$ 
coordinates of the point $\mathbf{X_{world}}=[X_i, Y_i, Z_i]^T$ in World Coordinate System
are projected as $\mathbf{x_{img}}=[u_k,v_k,1]^T$ pixel coordinates of the IR camera. 
To do so, we require the similar projection function to RGB camera 
(see section \ref{sb_sc_pinhole})
consists of intrinsic and extrinsic camera parameters, but this time it is for 
the IR camera:

\begin{equation}
  \mathbf{X_{world}} =
  Z_i (\mathbf{P_{IR}^{-1}}\mathbf{X_{world}}) = 
  Z_i (\mathbf{K_{IR}^{-1}}\mathbf{T_{IR}^{-1}}\mathbf{x_{img}})
\end{equation} \label{eq:ir_cam_proj_func}


%One can also retrive remaining $X_k$ and $Y_k$ coordinates of point $k$ in the Camera 
%Coordinate system if coordinates of the principal point $(c_x, c_y)$ of the 
%IR camera and its distortion paramaters $(\delta x, \delta y)$ are calibrated, and 
%$u_k$ and $v_k$ coordinates of the point $k$ in the image plane is measured:
%
%\begin{equation}
%\begin{aligned}
%  X_k = -\frac{Z_k}{f} (u_k - c_x + \delta x)\\
%  Y_k = -\frac{Z_k}{f} (v_k - c_y + \delta y)
%\end{aligned}
%\end{equation}

To find related intrinsic parameters for RGB and depth camera, we will perform 
calibration process in the following section.

\section{RGB-D Camera Calibration} \label{sb_sc_rgb_calibration}

The calibration process is a crucial part of any computer vision applications 
and there are many sophisticated techniques to achieve accurately.
However, it is important to note that full derivations of the calibration formulation 
is not provided, but only the important points are given.
Therefore, I refer readers to \cite{bla} for the rgb camera calibration and 
to \cite{bla} for the depth camera calibration. 
Since we are about to calibrate two cameras; i.e., RGB and depth, that we 
used to register each 3D point cloud with a color, we assume that both camera's image 
planes are aligned and it has 1:1 pixel correspondences. Under this assumption,
let's start with RGB camera.

\subsubsection{RGB Camera}

Assuming that we project two 3D points onto our image plane
like in Figure-\ref{fig:calibration_constraint},
the visual angle between those pair of 3D points is equal to 
the angle between its corresponding 2D points. This geometric constraints 
allow us to build the calibration problem. Remember that the goal is to find 
the projection matrix and we can build the problem in the following way:

\begin{figure}[H]
	\centering
  \includegraphics[width=\linewidth,natwidth=640,natheight=640]
  {fig/ref_imgs/calibration_dlt.png}
  \caption{Inherit Constraint}
  \label{fig:calibration_constraint}
\end{figure}


\begin{equation}
  \mathbf{x_{img}} = 
  \begin{pmatrix}
    u_i\\
    v_i\\
    1
  \end{pmatrix}
  =
  \mathbf{P}\mathbf{X_{world}} = 
  \begin{bmatrix}
    p_{00} & p_{01} & p_{02} & p_{03}\\
    p_{10} & p_{11} & p_{12} & p_{13}\\
    p_{20} & p_{21} & p_{22} & p_{23}
  \end{bmatrix}
  \begin{pmatrix}
    X_i\\
    Y_i\\
    Z_i\\
  \end{pmatrix}
\end{equation} \label{eq:proj_matrix}

Let's now distribute 
the projection matrix onto the 3D point measurement to retrieve individual 
pixel coordinates:

\begin{equation}
  u_i = 
  \frac
  {p_{00}X_i + p_{01}Y_i + p_{02}Z_i + p_{03}}
  {p_{20}X_i + p_{21}Y_i + p_{22}Z_i + p_{23}}
\end{equation} \label{eq:u_i}

\begin{equation}
  v_i = 
  \frac
  {p_{10}X_i + p_{11}Y_i + p_{12}Z_i + p_{13}}
  {p_{20}X_i + p_{21}Y_i + p_{22}Z_i + p_{23}}
\end{equation} \label{eq:v_i}


Since $\mathbf{x_{img}}$ and $\mathbf{X_{world}}$ are known,
we can find elements of the $\mathbf{p} = (p_{00}, p_{01}, \dots, p_{23})^T$ matrix 
by solving $\mathbf{Ap=0}$ liner system of equations from \ref{eq:u_i} and \ref{eq:v_i}.

For \textit{minimal solution} of this linear system of equations, 
we need at least $n \geq 6$ measurement points to solve the problem as 
the $\mathbf{P}$ matrix has 12 degree of freedom (11 if scale is ignored).
Note that this accounts for having 
noise-free measurement which does not hold in reality. Then, the problem 
becomes \textit{over-determined}.

In noisy measurement case, the problem is usually solved with
\textit{singular value decomposition (SVD)} with $n \geq 6$ measurement points. 
This method is called the \textit{Direct Linear Transformation (DLT)}.
Disadvantange of the DLT methods, it is still sensitive errors since 
it only considers \textit{algebraic errors}, which are the residuals of 
$\mathbf{Ap}$. 

While estimating the intrinsic and extrinsic parameters 
that are in linear form with the DLT, 
another issue known as \textit{radial distortion} 
has to take into account as well. This issue is caused by camera lens 
and Figure-\ref{fig:cam_distortion} depicts this effect and the straight lines 
appear to be curved. 

\begin{figure}[H]
	\centering
  \includegraphics[width=0.3\linewidth,natwidth=640,natheight=640]
  {fig/ref_imgs/cam_distortion.png}
  \caption{Camera Distortion}
  \label{fig:cam_distortion}
\end{figure}

This distortion has non-linear characteristics and we also 
need to estimate the coefficients 
$\kappa = (k_1, k_2, p_1, p_2, k_3)^T$ of the following polynomial:


\begin{equation}
\begin{split}
  x''_i = F_{dist}(x') = 
  x'(1+ k_1 r^2 + k_2 r^4 + k_3 r^6) + 2 p_1 x' y' + p_2 (r^2+2x'^2)\\
  y''_i = F_{dist}(y') = 
  y'(1+ k_1 r^2 + k_2 r^4 + k_3 r^6) + p_1 (r^2+2y'^2) + 2p_2 x'y'\\
\end{split}
\end{equation}

where $x' = X_{cam}/Z_{cam}$ and $y' = Y_{cam}/Z_{cam}$. 
Note that, in \ref{eq:simplyfied_proj_func}, we first transform 
3D points from World Coordinate to Camera Coordinate Systems with 
extrinsic matrix and then we project them with the instrinsic matrix. 
To improve our pinhole camera model, 
we need to distort 3D points in the Camera Coordinate System before 
multiplying with the intrinsic matrix:

\begin{equation}
  \begin{pmatrix}
    U\\
    V\\
    1
  \end{pmatrix}
  =
  \begin{pmatrix}
    f_x x'' + c_x\\
    f_y y'' + c_y\\
    1
  \end{pmatrix}
    =
    F_{dist}\begin{pmatrix}
      \mathbf{K}
      \begin{pmatrix}
        X_{cam}\\
        Y_{cam}\\
        Z_{cam}\\
        1
      \end{pmatrix}
    \end{pmatrix} 
    =
    F_{dist}\begin{pmatrix}
      \mathbf{K} [\mathbf{R}|\mathbf{t}]
      \begin{pmatrix}
        X_{world}\\
        Y_{world}\\
        Z_{world}\\
        1
      \end{pmatrix}
    \end{pmatrix}
\end{equation} \label{eq:proj_func_w_f_c}

Above operations introduce non-linearity, which cannot be solved by DLT.
To get better accuracy at our projection matrix along with the distortion, 
\textit{least squares estimation}, i.e., Levenberg-Marquardt, is perfomed.

\begin{figure}[H]
	\centering
  \includegraphics[width=0.5\linewidth,natwidth=640,natheight=640]
  {fig/ref_imgs/checkerboard_rgb.png}
  \caption{Checkerboard RGB}
  \label{fig:checkerboard_rgb}
\end{figure}

In practice, the checkerboard is used to get many good measurement points 
as  we easily extract edge features from the image. Now, the DLT comes handy
as we can use the DLT's result as an initial guess 
in the least squares optimization problem.

\begin{equation}
  \argmin_{\mathbf{P_{RGB}} \rightarrow \mathbf{K_{RGB}}, \kappa_{RGB}, \mathbf{R_{RGB}}^{(i)}, \mathbf{t_{RGB}}^{(i)}}
  \sum_{i=1:n} || \mathbf{x_{img}}^{(i)} - 
  \mathbf{P_{RGB}} \mathbf{X_{world}}^{(i)} ||^2
\end{equation}\label{eq:proj_lsq}

where we use known 
$\mathbf{x}_i$ is the pixel coordinates of a feature on image plane and 
$\mathbf{X}_i$ is the coordinates of a feature in World Coordinate system
to identify the following parameters:

\begin{itemize}
  \item $\mathbf{K_{RGB}}$ is the intrinsic matrix for RGB camera, 
  \item $\kappa_{RGB}$ is the distortion coefficients for RGB camera, 
  \item $\mathbf{R_{RGB}}^{(i)}$ is the corresponding orientation for RGB camera and 
  \item $\mathbf{t_{RGB}}^{(i)}$ is the corresponding translation for RGB camera.
\end{itemize}

\subsubsection{Depth Camera}

The depth camera almost has an identical procedure to RGB camera with minor 
differences. 
To begin with the calibration process, 
we, similarly, capture the depth image of checkerboard at the same position 
with RGB camera and detect the same features as shown in the following figure.

\begin{figure}[H]
	\centering
  \includegraphics[width=0.5\linewidth,natwidth=640,natheight=640]
  {fig/ref_imgs/checkerboard_depth.png}
  \caption{Checkerboard Depth}
  \label{fig:checkerboard_depth}
\end{figure}

The difference is, however, in the cost fuction of least squares equation. 
For RGB camera, we minimize the error on the image plane (see \ref{eq:proj_lsq}) 
by projecting 3D points in world onto image plane. Whereas, for depth camera, 
we will minimize the error in the World Coordinate system since we need to 
include depth $Z_i$ into the calibration process (remember \ref{eq:depth_w_disparity}).


\begin{equation}
  \argmin_{\mathbf{P_{IR}} \rightarrow \mathbf{K_{IR}}, \kappa_{IR}, \mathbf{R_{IR}}^{(i)}, \mathbf{t_{IR}}^{(i)}}
  \sum_{i=1:n} || \mathbf{X_{world}}^{(i)} - 
  Z_i (\mathbf{P_{IR}^{-1}} \mathbf{x_{img}}^{(i)}) ||^2
\end{equation}\label{eq:proj_lsq}

where we use known 
$\mathbf{x_{img}}^{(i)}$ is the pixel coordinates of the same feature on image plane and 
$\mathbf{X_{world}}^{(i)}$ is the coordinates of the same feature in World Coordinate system
to identify the following parameters:

\begin{itemize}
  \item $\mathbf{K_{IR}}$ is the intrinsic matrix for IR camera, 
  \item $\kappa_{IR}$ is the distortion coefficients for IR camera, 
  \item $\mathbf{R_{IR}}^{(i)}$ is the corresponding orientation for IR camera and 
  \item $\mathbf{t_{IR}}^{(i)}$ is the corresponding translation for IR camera.
\end{itemize}


The afromentioned calibration processes are well-studied problem in 
computer vision literature. 
Fortunately, there are many software libraries, such as OpenCV, 
that offers such implementations.


\chapter{Fundamentals of Visual Odometry} \label{cp_vo}

\section{Related Works} \label{sc_visual_odometry_related_works}



\section{Visual Odometry Pipeline} \label{sc_visual_odometry_pipeline}

\begin{figure}[H]
	\centering
	\includegraphics[width=\linewidth,natwidth=640,natheight=640]
  {fig/ref_imgs/vo_pipeline.png}
  \caption{Visual Odometry Pipeline}
	\label{fig:visual_odometry_pipeline}
\end{figure}



\section{Image Features} \label{sc_img_features}

Image features are a collection of regions of interest or of points of interest 
that describe the image. In this way, we compress the necessary information 
from images so that we can achieve computationaly expensive task more efficiently.
Points of interest, also called \textit{keypoints}, 
are particularly valuable because their location in the image can be 
measured accurately. This is useful for localization related tasks such as VO. 

\subsection{Feature Extraction} \label{sb_sc_feature_extraction}

The main goal is to find good features in feature-based VO. 
What defines a good feature is that it must be distinct, 
repeatable, computationaly cheap and invariant to geometrical changes. 
Obviously, one has many options to produce such image 
feautes but two common methods that are widely used in VO systems are 
blobs and corners. 
Blobs are image patterns that contain distinct image response comparing to their 
neighborhood pixels. Blobs take advantage of pixel intensity or color to 
decide whether it has a distinct response or not.
In the VO literature, SIFT\cite{}, SURF\cite{} or CENSURE\cite{} are popular 
choices for detecting blob featurus.
Corners are the meeting points where two or many edges intersect. Corners, 
on the other hand, 
take advantage of geometrical structure of an image. FAST\cite{}, Shi-Tomasi or 
Haris\cite{} are widely used for detecting corners.

Fundemantally, a two-step process is needed to extract good features. 
First, you take a response function called \textit{image filter}, 
shift this filter through the image and save the one that have greater 
response than your previously defined threshold as Keypoints. 
This might a Gaussian filter for blobs or a 
corner detector filter for corners. In the second step (optional), 
you perform non-maxima 
suppression on the resulting keypoints to find local minima of the function. 
This step will help to remove similar keypoints and to choose the ones having 
maximum confidence so that distinctiveness of the features are ensured.

Inheritly, each feature detector has certain limitations and one has to 
choose whihc detector to use according to task objectives. Therefore, we may ask 
the following question: 
Does the localization envorinment involve more texture oriented objects like 
floors, walls etc. or geometrical shapes like urban areas where many lines 
exist?
However, rule of thumb when choosing feature detector is that blobs 
are distinct but slow to compute and corners are fast to compute but less 
distict. 

\subsection{Feature Descriptor} \label{sb_sc_feature_descriptor}

Extracting features is the very first step of VO. The next step is to 
encode the detected keypoints into a format that we can perform comparison 
or search operation among them. This is done by taking the neighboring pixels 
around the keypoints and convert into a more compact form. For example, SIFT 
, most well-known feature decsriptor, 
creates a patch around a keypoint, divides this patch into smaller grids,
calculate the gradient of each grid and 
saves them as a histogram.
This procedure makes feature descriptor 
robust agaist scale or rotation changes. Then, one can use these descriptors for 
various comparison operations such as matching or tracking in VO. However, we 
utilize the ORB in our VO and will discuss its properties.


\subsubsection{ORB} \label{sb_sc_orb}

One of the most strict requirements of VO is the real-time contraints since it is 
expected to work at similari to low-level inertial sensors, i.e., accelerometer, gyroscopes etc. 
As previously discussed, blobs detectors are computationaly expensive. Therefore, 
corners based feature detector are more prevelant in VO. 
\textit{ORB (oriented FAST and rotated BRIEF)} studied known issues of 
FAST detector and BRIEF descriptor. Then, it combines them to compansate each 
other's drawbacks. In the end,
it perform as accurate as SIFT, plus faster. Here are steps on how to extract features 
and create descriptors with ORB: 

\begin{enumerate}
  \item \textbf{Detect corners with FAST}: FAST take each pixel on 
    the image and compare with its adjacent pixels. More specificially, 
    ORB uses FAST-9, which takes a patch of discrete circular radius of $r=9$. 

    \begin{figure}[H]
	\centering
  \includegraphics[width=0.6\linewidth,natwidth=640,natheight=640]
  {fig/ref_imgs/fast.png}
  \caption{FAST corners}
  \label{fig:fast_corners}
\end{figure}

    If the 
    selected pixel is $\pm t$ darker or brighter than adjacent pixels, 
    we call it a corner, where $t$ is our empiric threshold.

\begin{equation}
  M_c(p) = 
  \sum_{r \in S_{bright}} |I_{p \rightarrow r} - I_p| - t \text{ or } 
  M_c(p) = 
  \sum_{r \in S_{dark}} |I_p - I_{p \rightarrow r}| - t)
\end{equation}
    We retrieve $M^c_{1:n}$ the set of corner candidates from 
    comparing $I_{p\rightarrow x}$ the adjacent pixels around $I_p$ the target pixel.

  \item \textbf{Rank detected keypoint with Harris}: After FAST detection, 
    we get many corner candidates around the interest point. However, 
    FAST does not measure how good a corner is. Thus, we use Harris corner 
    detector to rank corner candidates:
    \begin{equation}
      \mathbf{A} = \sum_{x,y} w(x,y) 
      \begin{bmatrix}
        I_x^2 & I_xI_y \\ I_xI_y & I_y^2
      \end{bmatrix}
    \end{equation}
    The $\mathbf{A}$ matrix is calculated by the $I_x$ and $I_y$ partial 
    derivatives with respect to x and y direction and $w(x,y)$ weighting window.

    \begin{equation}
      R^c = det(\mathbf{A}) - k(trace(\mathbf{A}))^2
    \end{equation}
    where $det(\mathbf{A}) = \lambda_1 \lambda_2$ and 
    $trace(\mathbf{A}) = \lambda_1 + \lambda_2$.
    Then, we use the resulting $\mathbf{A}$ to find a ranking score for each 
    corner. Now, it is possible to take top N corners if desired.

  \item \textbf{Calculate orientation of corners with image moments}: 
    ORB uses BRIEF to create feature descriptors; however, BRIEF fails in 
    rotated images. Therefore, ORB modifies the BRIEF by adding orientation 
    information. To get orientation, an \textit{image moment} are calculated
    for each patch $\mathbf{S_n}$:

    \begin{equation}
      m_{a,b}(\mathbf{S_n}) = \sum_{x,y \in \mathbf{S_n}} x^a y^b I(x,y)
    \end{equation}
    where $a + b$ defines the order of the moment and we need 
    the moments of order one:

    \begin{equation}
      m_{1,0}(\mathbf{S_n}) = \sum_{x,y \in \mathbf{S_n}} x \cdot I(x,y) \text{  ,  }
      m_{0,1}(\mathbf{S_n}) = \sum_{x,y \in \mathbf{S_n}} y \cdot I(x,y)
    \end{equation}

    Then, we get the orientation of the patch $\mathbf{S_n}$:
    \begin{equation}
      \theta(\mathbf{S_n}) = atan2(m_{01}, m_{10})
    \end{equation}

  \item \textbf{Form BRIEF descriptors with their corresponding orientation}:
    Once the top N corners and their orientations are detected, descriptions 
    can be formed with BRIEF. 

\begin{figure}[H]
	\centering
  \includegraphics[width=\linewidth,natwidth=640,natheight=640]
  {fig/ref_imgs/brief.png}
  \caption{BRIEF pairs}
  \label{fig:brief_pairs}
\end{figure}


    To do so, we randomly (normal distribution) 
    selected 256 pairs $(p_i,q_i)$ 
    inside the patch $\mathbf{S_n}$:
    \begin{equation}
      \mathbf{S_n} = 
      \begin{pmatrix}
      p_0, \dots, p_n\\
      q_0, \dots, q_n
      \end{pmatrix}
    \end{equation}
    Next, we rotate each $(\mathbf{p_i, q_i})$ pair points in 
    $\mathbf{S_n}$ with the 
    corresponding corner's orientation:
    \begin{equation}
      \mathbf{p}_{i,\theta} = \mathbf{R}_{\theta}\mathbf{p_i} \text{ and } 
      \mathbf{q}_{i,\theta} = \mathbf{R}_{\theta}\mathbf{q_i}
    \end{equation}
    where $p_i=(x_i, y_i)$ and $q_i=(x_i, y_i)$ are the pixel coordinates of 
    the points.
    It is important to note that authors \cite{} suggested to rotate each point in 
    increments of 2$\pi$/30. Therefore, orientaion $\theta$ is mapped to 
    nearest multiple of 2$\pi$/30.

    To form steered (or rotated) BRIEF descriptors, we perform pixel density comparison:
    between randomly selected pair points:
    \begin{equation*}
      \tau(\mathbf{p_{i,\theta}},\mathbf{q_{i,\theta}}) := 
      \begin{cases}
        1  & I(\mathbf{p_{i,\theta}}) < I(\mathbf{q_{i,\theta}}),\\
        0  & I(\mathbf{p_{i,\theta}}) \geq I(\mathbf{q_{i,\theta}})
      \end{cases}
    \end{equation*}

    Finally, we sum comparison results with binary form to get the desctriptor 
    of the patch $\mathbf{S_n}$:
    \begin{equation}
      \mathbf{D_n} = f(\mathbf{S_n}) := \sum_{1\leq i \leq n} 2^{i-1}\tau (\mathbf{p_{i,\theta}, q_{i,\theta}})
    \end{equation}

\end{enumerate}





\subsection{Feature Matching} \label{sc_feature_matching}

Now that we know how to extract a distinct feature and to form a descriptor, 
we can start building a relationship across images to estimate how the camera 
moves. Remember that 
the camera can procedure
a video stream consisting of usually ranging from 30 to 60 frames per second. 
The first task is to form a group of image pairs and to compute translation and rotation 
information between each image pair, continuously. In literature, there are two ways to 
select image pairs: 
frame to frame or key frame to frame. In the former case, one pairs consecutive 
frames across video stream. In the latter case, one selects a reference 
frame and keep matching it with subsequent frames as long as the pair has 
sufficient amount of feature matchings. The latter has certain advantages over 
the former; however, we choose the former as we wish to model uncertainty 
of our motion estimation algorithm as accurate as possible. 

\begin{figure}[H]
	\centering
	\includegraphics[width=\linewidth,natwidth=640,natheight=640]
  {fig/ref_imgs/feature_matching.png}
  \caption{Feature Mathces}
	\label{fig:feature_matchings}
\end{figure}


All being said, 
let's assume we pair consecutive images
$(\mathbf{I}_k, \mathbf{I}_{k+1})$. 
In each image, we gather feature descriptors 
$(\mathbf{D}_k^{1:n}, \mathbf{D}_{k+1}^{1:m})$ that pass through our ORB 
filter. The goal is to find the feature correspondences based on their 
rotated BRIEF descriptor values. Again, there are many efficient ways to 
perform this matching task such as FLANN 
(Fast Library for Approximate Nearest Neighbors). 
Instead, in this thesis, we choose Brute-Force matching algorithm, which
produces less outliers but it is less efficient in terms of time complexity.
The Brute-Force, as its name states, is a straightforward technique
that compares each $\mathbf{D}_{k}^{1:n}$ descriptors in $k^{th}$ image 
with $\mathbf{D}_{k+1}^{1:m}$ descriptors in $k+1^{th}$ image by calculating 
\textit{Hamming} distance:

\begin{equation}
  d_h(\mathbf{D}_{k}^i,\mathbf{D}_{k+1}^j) = \mathbf{D}_{k}^i \oplus \mathbf{D}_{k+1}^j
\end{equation}

where $\oplus$ corresponds to an 'exclusive or' logic operation. After calculating 
Hamming distances, the minimum distance are kept as best matches. On top of that, 
we perform cross-check validation by ensuring that
matches with value $(i,j)$ such that $i^{th}$ descriptor in image $k$ 
has $j^{th}$ descriptor in image $k+1$ as the best match and vice-versa.


\subsection{RANSAC} \label{sb_sc_ransac}

In reality, not all feature matches are correct and it is critical that we 
detect wrong ones as the optimization algorithm that estimates the camera 
motion is sensitive to even small number of wrong matches. In technical terms,
we call these 
wrong matches \textit{outliers} (or \textit{false positives}). Hence, 
we need an algorithm to reject those outliers from \textit{inliers}. 
The most common way is to use RANSAC, which is an abbreviation to 
Random Sample Consensus. RANSAC is an interative algorithm which 
fits desired model with presence of outliers by selecting subset of dataset 
randomly and improving parameters of model each iteration. Note that 
RANSAC works 
well if at least half of the dataset contains inliers. 

\begin{figure}[H]
	\centering
	\includegraphics[width=\linewidth,natwidth=640,natheight=640]
  {fig/ref_imgs/line_ransac_outlier.png}
  \caption{Outlier Mathces}
	\label{fig:outlier_matches}
\end{figure}

In \ref{sb_sc_calibration}, we discussed how to model rotation and translation 
motion along with the intrinsic matrix. Parameteres that we aim to 
identify are elements of projection matrix in \ref{eq:proj_func_w_square_pix_skew}. 
Similarly, the model being fitted in RANSAC case is elements of a 
\textit{homography matrix}, 
which transforms a 2D image point to another 2D image point.
Remember that we have feature matches from image pairs. One of the pair is 
the transformed version of the other pair. 
We can model this relationship in the following way:

\begin{equation}
  k
  \begin{bmatrix}
    u_i' \\ v_i' \\1
  \end{bmatrix}
  =
  \mathbf{H}
  \begin{bmatrix}
    u_i \\ v_i \\1
  \end{bmatrix}
  =
  \begin{bmatrix}
    h_{00} & h_{01} & h_{02} \\
    h_{10} & h_{11} & h_{12} \\
    h_{20} & h_{21} & h_{22}
    \end{bmatrix}
  \begin{bmatrix}
      u_i \\ v_i \\1
    \end{bmatrix}
  \end{equation}\label{eq:homography_full}

The goal is to fit parameters of $\mathbf{H}$ with the selected subset of 
the matching and our assumption is majority of selected point matchings 
should be inliers. In this way, we can easily detect ourliers by testing 
whether they fit to model parameters or not. To make \ref{eq:homography_full} 
more obvious, we form linear system of equations:

\begin{equation}
\underbrace{
\begin{bmatrix}
  u_i & v_i & 1 & 0 & 0 & 0 & -u_i'u_i & -u_i'v_i -u_i'\\
  0 & 0 & 0 & u_i & v_i & 1 & -v_i'u_i & -v_i'v_i -v_i'
\end{bmatrix}}_{\mathbf{A}}
\underbrace{
\begin{bmatrix}
    h_{00} \\ h_{01} \\ h_{02} \\ h_{10} \\ h_{11} \\ h_{12} \\ h_{20} \\ h_{21} \\ h_{22}
\end{bmatrix}}_{\mathbf{h}}
= 
\begin{bmatrix}
  0\\0
\end{bmatrix}
\end{equation}

First step of RANSAC is to select a subset that contains 
minimum number of matching points to 
determine parameters of the model. In homography $\mathbf{H}$ case, we need at least 
4 point pairs $(\mathbf{p}=(u_i,v_i),\mathbf{q}=(u_i',v_i'))$ to solve 
this linear system of equation. However, due to the noise, we require more than 
4 point pairs but this makes the problem over-determined. Therefore, 
we might find an approximated solution by solving least squares problem:
With all that being said, we minimize the 
\textit{algebraic distance error}:

\begin{equation}
\argmin_h || \mathbf{Ah-0}||^2.
\end{equation}

In the second step, 
after estimating $\mathbf{h}$ paramaters, we test every matchings that are 
outside the subset which randomly selected at the first step whether 
they fit to the model with the certain $\mathbf{d}$ threshold we define:

\begin{equation}
  ||\mathbf{q} - \mathbf{H}\mathbf{p}||_2 > \mathbf{d} 
\end{equation}

In the third step, we include the points 
that passed our test procedure in the second step into our subset. In the 
forth step, we have another test. In this test, we check whether the number of 
matching points in our subset is large enough to prove that we include 
majority of the inliers. If not, we go back to first step and repeat the 
whole process again until we fulfill the fourth step. Here is the pseudo code 
that summarize the algorithm:

\begin{algorithm}[H]
  \caption{Rejecting outlier matches with RANSAC}\label{algo_ransac}

  \textbf{Input} \\
    \hspace*{\algorithmicindent}\textbf{S}: the smallest number of points\\
    \hspace*{\algorithmicindent}\textbf{N}: the number of iteration\\
    \hspace*{\algorithmicindent}\textbf{d}: the threshold used to identify a point which fits the model\\
    \hspace*{\algorithmicindent}\textbf{T}: the number of nearby points to notify that there is a good fit\\
  \textbf{Output} \\
    \hspace*{\algorithmicindent}\textbf{C}: the (consensus) set of inliers \\
  \begin{algorithmic}[1]

    \Procedure{RANSAC}{\textbf{S,N,d,T}}\\
      \While{iterations $<$ \textbf{N}}
      \State select random sample subset of \textbf{S} points
      \State estimate parameters to fit homography with \textbf{S}
      \ForEach {points outside \textbf{S}}
        \State calculate error between estimated point and measured point
        \If {error $<$ \textbf{d}}
          \State add point into \textbf{S}
        \EndIf
        \If {\textbf{S} $>$ \textbf{T}}
          \State return \textbf{C} $=$ \textbf{S}
        \EndIf
      \EndFor
      \EndWhile
    \EndProcedure
  \end{algorithmic}

\end{algorithm} \label{algo:ransac}

As it is seen in \ref{algo:ransac}, there are number of empiric parameters 
that we need to define; $\mathbf{S,N,d,T}$. To make the algorithm as 
efficient as possible, these parameters must be chosen carefully. As we 
discussed, $\mathbf{S}$ is the subset of matchings that we randomly select and 
initial value should at least 4 so that we can solve the least 
squares problem. 

For $\mathbf{N}$, it is insufficient to iterate 
through every matching points. Thus, we at least select $\mathbf{N}$ 
number of mathcing points with respect to following condition:

\begin{equation}
  \mathbf{N} = log(1-p)/log(1-(1-\epsilon)^s)
\end{equation}

where $p=0.99$ is the probability of covering all inliers, $s$ is the 
minimum number of iteration that 
probability of choosing a subset with only outliers 
and $\epsilon$ is the probability that a matching is an outlier.

For $\mathbf{d}$, it is chosen empirically if distribution of outliers is unknown. If 
it is known, i.e, Gaussian with mean $\mu$ and $\sigma$, threshold should 
be $d=5.99\sigma^2$ so that there is a 95\% probability that the point is an inlier.

For $\mathbf{T}$, we might have a case that we reach expected ratio of inliers; thus 
we don't have to iterate through $N$ number of times. We can terminate it 
ealier if the following condition is satisfied:

\begin{equation}
  \mathbf{T} = (1-\epsilon)n
\end{equation}

where $n$ is the total number of matching points.

It is crucial 
to note that we may still have outliers within $\mathbf{d}$ threshold after 
RANSAC.
However, 
our motion estimation will be greatly improved since 
majority of the outliers are removed.
Finally, we will discuss how we can utilize the carefully selected features 
and its matches to estimate the camera motion.


\section{Motion Estimation} \label{sc_motion_estim}

SWITCH K AND C INDICES ON TRANSLATION AND ROTATION RESERVE!

Motion estimation is the core part of the VO system. After 
extracting and matching features,
we finally are ready to compute $\mathbf{T}_{k, k+1}$ transformation matrix. 
The \textit{transformation matrix} corresponds to relative camera motion information between 
two images that are recorded in different poses (see figure-\ref{fig:transformation_ij}).
Let's assume; we have 
consecutive camere poses $\mathbf{x}_k^c = [\mathbf{p}_k^c, \mathbf{q}_k^c]$ and 
$\mathbf{x}_{k+1}^c = [\mathbf{p}_{k+1}^c, \mathbf{q}_{k+1}^c]$ where 
$\mathbf{p}^c = [p_x, p_y, p_z]^T$ is the position of the camera in $\R^3$
and $\mathbf{q}^c = [q_w, q_x, q_y, q_z]^T$ is 
the orientation of the camera in quaternion form in $SO(3)$. 
Notice that, in camera model chapter 
\ref{cp_cam_models}, we used rotation matrix to represent orientations, 
which made it covenient to combine intrinsic matrix and extrinsic matrix 
(rotation and translation) into a single projection matrix 
(refer to notation \ref{eq:simplyfied_proj_func})
so that 
we optimize for paramaters of the single matrix. On the other hand, when 
estimating relative motion, we use quaternions to represent orientations, 
which is less intuitive way but 
has certain advantage over rotation matrix such as requiring less storage.
Additionally, we can represent the transformation 
$\mathbf{T}_{k,k+1}$ $(= [\mathbf{t}_{k,k+1},\mathbf{q}_{k,k+1}])$ between two camera poses 
$(\mathbf{x}_k,\mathbf{x}_{k+1})$ with the rotation $\mathbf{q}_{k,k+1}$ in $SO(3)$  
and the translation $\mathbf{t}_{k,k+1}$ in $\R^3$.

\begin{figure}[H]
	\centering
  \includegraphics[width=0.7\linewidth,natwidth=640,natheight=640]
  {fig/ref_imgs/transformation_ij.png}
  \caption{GIVE EX FOR TRANSFORMATION TWO CAM POSES Transformation between Two Images}
	\label{fig:transformation_ij}
\end{figure}

That being said, one can formulate the transformation $\mathbf{T}$ 
between two camera pose. Let's first find the camera position:

\begin{equation}
  \mathbf{p}_{k+1} = 
  \mathbf{q}_{k,k+1} \otimes \mathbf{p}_k^c \otimes \mathbf{q_{k,k+1}}^* + 
  \mathbf{t}_{k,k+1}
\end{equation}

where $\mathbf{q}_{k,k+1} \otimes \mathbf{p}_k \otimes \mathbf{q_{k,k+1}}^*$ is the 
\textit{hamilton product} that is used to rotate the camera position at the $k^{th}$ pose 
and $\mathbf{t}_{k,k+1}$ is 
the simple vector addition that is used to translate the camera position. Next, 
the camera orientation can be found as follows:

\begin{equation}
  \mathbf{q}_{k+1}^c = 
  \mathbf{q}_{k,k+1} \otimes \mathbf{q}_{k}^c
\end{equation}

where $\mathbf{q}_{k,k+1}^c \otimes \mathbf{q}_{k}$ is the product of two 
quaternions that the former is the rotation and that the latter is the orientation 
of the camera at the $k^{th}$ pose.

\begin{figure}[H]
	\centering
  \includegraphics[width=0.7\linewidth,natwidth=640,natheight=640]
  {fig/ref_imgs/cam_trajectory.png}
  \caption{Camera Trajectory}
	\label{fig:cam_trajectory}
\end{figure}


The ultimate goal in VO is to compute transformation
$\mathbf{T}_{k,k+1}=[\mathbf{t}_{k,k+1},\mathbf{q}_{k,k+1}]$ in multiple consecutive 
images and concatenate them to build a trajectory of the camera.
As a consequence, we can track any agent on which the camera is placed rigidly. 
For example, concatenated transformation $\mathbf{T}_{0:n}$ 
can be used to calculate $n^{th}$ camera pose that is relative to the initial pose:


\begin{equation}
  \mathbf{p}_{n}^c = 
  \mathbf{q}_{n, n-1} \otimes (\dots
  (\mathbf{q}_{2,1} \otimes
  (\mathbf{q}_{1,0} \otimes \mathbf{p}_0^c \otimes \mathbf{q_{1,0}}^* + \mathbf{t}_{1,0})
  \otimes \mathbf{q}_{2,1}^* + \mathbf{t}_{2,1})
  \dots) \otimes \mathbf{q}_{n, n-1}^* +\mathbf{t}_{n, n-1} 
\end{equation}

\begin{equation}
  \mathbf{q}_{n}^c = 
  \mathbf{q}_{n,n-1} \otimes \dots \otimes \mathbf{q}_{2,1} \otimes \mathbf{q}_{1,0} \otimes \mathbf{q}_{0}^c 
\end{equation}

To find transformation, we take advantage of image features as they can 
inform us how the camera moves if we detect them across multiple frames.
All the afromentioned step such as feature extraction and feature matchings 
are performed so that we 
can compute relative motion.
Similar to projection matrix \ref{eq:proj_lsq} in camere calibration, we utilize 
the least squares method for estimating the best approximate transformation information
due to the noise. Several methods available in literature to create 
relationship between feature points that we detect across different image frames and 
we will discuss it in the next section.

\subsection{Relative Camera Pose Estimation Techniques}
\label{sb_sc_relative_camera_pose_estimation_techniques}

So far, we discussed how we can process images so that we have adequate 
information to compute camera pose. However, we only mentioned 2D image 
features. To esimate the pose in 3D world, we require corresponding 
depth information for features. Note that there are methods that 
retrieve relative scale information using only 2D image features and its epipolar 
constraints with monocular cameras, 
but we are interested in having a metric depth information rather 
than relative scale in this thesis. Therefore, we have two common choices in terms 
of camera types: Stereo Camera or RGBD Camera. In our experiments, we experimented 
on RGBD Camera to retrieve the depth information.

At this point, one generally has two ways to compute relative camere poses.
and the reason we have different kinds of way to compute transformation arises from 
the cost function we define for the least squres problem. In the end, all we wish to 
find a good model for our optimization problem so that we settle on the 
best possible local minimum.
The design choice for cost function comes from the fact that we build 
our cost function either on $\R^2$ space (image plane) or $\R^3$ space 
(Camere Coordinate system).
Therefore, in VO literature, 
there are 2 different cost functions for modeling the least problem:
\begin{itemize}
  \item 3D-to-2D correspondences,
  \item 3D-to-3D correspondences.
\end{itemize}
The 2D term refers to 2D points that are in the image plane and that we call 
them keypoints in section-\ref{sb_sc_feature_descriptor}. 
Whereas the 3D term refers to 3D points that are in the Camera Coordinate 
System and that we call them point clouds in section-\ref{sb_sc_feature_descriptor}.

NOTE: define keypoints and point clouds term if it is not defined in the feature extraction section.

Note that this thesis does not engage with 2D-to-2D correspondences method since it is 
used in monocular camera; thus it will not be discussed here. On the other hand, 
we will discuss and compare 3D-to-2D correspondences and 3D-to-3D correspondences. 
In the motivation chapter \ref{cp_motivation}, 
I state the reasons why I chose 3D-to-3D even though it is not a common choice 
in VO literature.


\subsubsection{3D-to-2D Correspondences}

Rememeber that,
after feature matching step for the consecutive image frames, we have only
2D-to-2D keypoint correspondences information and 
the transformation which we wish to compute is in $\R^3$. Therefore, we require 
transformation involving in 3D feature points.
That being said, we can estimate transformation 3D-to-2D Correspondences in four steps:

\begin{enumerate}
  \item back-project $k+1^{th}$ 2D keypoint to a 3D feature point
  \item back-transform the back-projected 3D feature point
  \item reproject the back-transformed 3D feature point onto $k^{th}$ keypoint
  \item minimize 2D \textit{reprojection error} along with all feature matches
\end{enumerate}

\begin{figure}[H]
	\centering
  \includegraphics[width=0.7\linewidth,natwidth=640,natheight=640]
  {fig/ref_imgs/min_euclidean_error.png}
  \caption{Minimize Reprojection Error}
	\label{fig:min_geometric_error}
\end{figure}


To illustrate the 
situation more clearly, let's assume we have a 3D feature point 
$\mathbf{X}_i=[x, y, z]^T$ in Camera Coordinate System and we measure 
the projections of this exact feature point as 2D keypoint features, 
$\mathbf{x}_i^k=[u_i^k,v_i^k]^T$ and $\mathbf{x}_i^{k+1}=[u_i^{k+1}, v_i^{k+1}]^T$ 
on subsequent camera poses $k^{th}$ and $k+1^{th}$ respectively. What we also know is that 
we can back-project measured 2D keypoints to 3D feature points using projection 
matrix \ref{eq:simplyfied_proj_func_1} that we estimated in calibration process. 
Note that, since we measure 3D feature points with respect to Camera coordinate system, 
we are not interested in extrinsic matrix but only intrinsic. Now, 
let's write again projection function that converts 3D feature 
points to 2D image keypoints:
$
\mathbf{x}_i = 
  \mathbf{K}\mathbf{X}_i
  $.
One can also back-project 2D image keypoints to 3D feature points:
$
  \mathbf{K}^T\mathbf{x}_{i} = 
  \mathbf{X}_{i} 
  $.

Now, we can formulate the four steps of 3D-to-2D correspondences as follows:

\begin{enumerate}
  \item $\mathbf{X}_{i}^{k+1} = \mathbf{K}^T\mathbf{x}_{i}^{k+1}$
  \item $\mathbf{X}_i^{k'} = 
    \mathbf{q}_{k,k+1} \otimes \mathbf{X}_i^{k+1} \otimes \mathbf{q}_{k,k+1}^* + \mathbf{t}_{k,k+1} $
  \item $\mathbf{x}_i^{k'} = \mathbf{K}\mathbf{X}_i^{k'}$
  \item minimize $\sum_i||\mathbf{x}_i^k - \mathbf{x}_i^{k'}||_2^2$
\end{enumerate}

The second and third steps can be encapsulated to a function $f$ so that 
we formulate the whole optimization problem in the following form:

\begin{equation}
  \argmin_{\mathbf{T} = [\mathbf{t}_{k,k+1}, \mathbf{q}_{k,k+1}]} = 
  \sum_i||\mathbf{x}_i^k - f(\mathbf{T}, \mathbf{X}_i^{k+1})||^2_2
\end{equation}

This method works considerably well in practice. In VO literature \cite{}, 
it is also reported that it performs better than 3D-to-3D correspondences.

\subsubsection{3D-to-3D Correspondences}

Another way of modeling the cost function is to utilize only 3D feature point 
correspondences and one can estimate transformation with 3D-to-3D correspondences in three steps:

\begin{enumerate}
  \item back-project both $k^{th}$ and $k+1^{th}$ 2D keypoints to 3D feature points
  \item back-transform back-projected $k+1^{th}$ 3D feature point
  \item minimize 3D \textit{euclidean distance} along with all feature matches
\end{enumerate}

\begin{figure}[H]
	\centering
  \includegraphics[width=0.7\linewidth,natwidth=640,natheight=640]
  {fig/ref_imgs/min_euclidean_error.png}
  \caption{Minimize Euclidean Error}
	\label{fig:min_euclidean_error}
\end{figure}


Similar illustrations that we previously did for 3D-to-2D correspondences applies 
for the 3D-to-3D correspondences as well but only with few changes. 
Let's again assume that we have two corresponding 2D keypoint features,
$\mathbf{x}_i^k$ and $\mathbf{x}_i^{k+1}$. However, rather minimizing error on 
2D image plane, we want to minimize them on 3D Camera coordinate system. Therefore, 
we need to back-project both keypoint features. With that in mind, 
we can formulate the three steps of 3D-to-3D correspondences as follows:

\begin{enumerate}
  \item $\mathbf{X}_{i}^{k+1} = \mathbf{K}^T\mathbf{x}_{i}^{k+1}$ and 
    $\mathbf{X}_{i}^{k} = \mathbf{K}^T\mathbf{x}_{i}^{k}$ 
  \item $\mathbf{X}_i^{k'} = 
    \mathbf{q}_{k,k+1} \otimes \mathbf{X}_i^{k+1} \otimes \mathbf{q}_{k,k+1}^* + \mathbf{t}_{k,k+1}$
  \item minimize $\sum_i||\mathbf{X}_i^k - \mathbf{X}_i^{k'}||_3^2$
\end{enumerate}

The second step can be encapsulated to a function $g$ to form the optimization 
problem:

\begin{equation}
  \argmin_{\mathbf{T} = [\mathbf{t}_{k,k+1}, \mathbf{q}_{k,k+1}]} = 
  \sum_i||\mathbf{X}_i^k - g(\mathbf{T}, \mathbf{X}_i^{k+1})||^2_3
\end{equation}

In VO literature, this method is usually discarded since 
it performs poorly comparing to 3D-to-2D correspondences.

NOTE: State why it works poorly? Is it bc back-projected 3D feature point more noisy?

% ***************************CP3-MOTIVATION***************************
\chapter{Motivation} \label{cp_motivation}

COPIED:

The papers dealing with uncertainty in RGB-D sensors focus mostly on 
applications outside of VO/SLAM, but Park et al. 
[36] proposed a mathematical uncertainty model for Kinect v1 sensor and RGB-D 
sparse point features. However, the approach of [36] was 
demonstrated without an application in real SLAM or VO. 

% ***************************CP4-CoVO***************************
\chapter{An Error-Aware RGB-D Visual Odometry} \label{cp_covo}

\section{Related Works} \label{sc_error_aware_visual_odometry_related_works}

\section{Modeling Errors in VO} \label{sc_spatial_uncertainty}

The main reason why conventional VO applications do not provide any uncertainty 
information (namely covariance matrix) is that it is hard to model 
error characteristics of the whole VO pipeline as we perform many preprocessing, 
each of which eventually introduces different types of error. What we aim 
, in this chapter, is to define potential error source of the VO and 
to model them. To do so, we will investigate noise characteristic of 
sensors in RGB-D camera. 
Kinect, having three sensors: RGB camera, IR camera
and Ifrared (IR) laser projector as we discussed in chapter \ref{cp_cam_models}. 
In our experiments and evaluations,
we assume that these sensors are calibrated such that 
there are no registrations error when mapping RGB pixels to disparity pixels. 
Furthermore, 
we assume that measurements with these sensors are independent of each other. 
Under this asssumption, 
we split source of errors into two categories: feature related 
and depth related uncertainties.
The former is caused by feature extraction and matching algorithms. 
The latter is caused by depth camera sensor. 
In the following sections, we will discuss how we can model these 
two error sources and how to form an uncertainty model 
for the Kinect so that we estimate meaningful covariance matrices for each 
relative camera pose.

\subsection{Feature Related Uncertainties} \label{sb_sc_pixel_uncertainty}

Our VO pipeline heavily relies on the detected features 
(also called landmarks). When building such a VO system, 
it is expected that you will have a video stream that has small translation 
and rotation differences at the consecutive images. Thus, when pairing images 
to find common feature, we expect not to have high-degree rotation or large 
amount of scaling on image features so that matching algorithm would not suffer 
from high number of outliers. Under this circumstances, we identify two 
main error sources related to features; i.e., interest point location uncertainty 
and outliers in feature matching.


To understand these two types of errors, we need to remind ourselves 
how we detect and describe features section \ref{sb_sc_orb} in the first place. 
In ORB, FAST corner filter is performed by selecting pixel coordinates of an 
interest point and comparing it with its surrounding pixels. In ideal case 
where we match features in consecutive image perfectly, we would assume that 
the average error is half pixel due to the dicretization process of the RGB camera. 
However, in reality, we still have outliers even after applying RANSAC. 
What we also know that we can set a certain RANSAC threshold keep the outliers 
within certain error boundaries. If RANSAC holds its promise, we could 
assume the outliers will be within the RANSAC threshold. Then, outliers 
become \textit{fake} inliers that has bigger interest point location uncertainty than 
the \textit{real} inliers. At this point, we have make an approximation over 
feature related uncertainty. In practice, RANSAC threshold would be set 
between 1-10 pixel which already greater than uncertainty of the interest point 
location uncertainty (which was half pixel). Therefore, we might neglect 
interest point location uncertainty and use RANSAC threshold value for 
calculating uncertainty of each feature.

\begin{figure}[H]
	\centering
  \includegraphics[width=0.7\linewidth,natwidth=640,natheight=640]
  {fig/ref_imgs/feature_related_uncertainty_one.png}
  \caption{Feature Related Uncertainty}
	\label{fig:feature_related_uncertainty_one}
\end{figure}


\begin{figure}[H]
	\centering
  \includegraphics[width=0.7\linewidth,natwidth=640,natheight=640]
  {fig/ref_imgs/feature_related_uncertainty_many.png}
  \caption{Feature Related Uncertainty with Many Features}
	\label{fig:feature_related_uncertainty_many}
\end{figure}



Having all these in mind, let's model uncertainty related to features.
First of all, remember that 3D points in world are projected to 2D points on an image plane with 
Pinhole Model from section-\ref{sb_sc_pinhole}. When the aperture of a digital 
camera opens, it captures incoming light rays using its light detector sensor 
(typically CMOS image sensors) and turns them into electrical signals. That 
is an over-simplfied definition of a color camera. In addition to pinhole 
model, one can build a model for light ray coming from a 3D point.
. We call this model 
a \textit{conic ray} model which is a geometrical representation of 
single projection operation. i
Before we describe the conic ray model, let's consider a simple example where we 
project 2D $\mathbf{x}$ position of a point cloud onto 1D $\mathbf{y}$ position
as it is seen in figure-\ref{fig:conic_ray_2d_error_model}.


\begin{figure}[H]
	\centering
  \includegraphics[width=0.7\linewidth,natwidth=640,natheight=640]
  {fig/ref_imgs/conic_ray_2d_model.png}
  \caption{Conic Ray 2D Error Model}
	\label{fig:conic_ray_2d_error_model}
\end{figure}

Due to the errors in detected and matched features, 
we measure the position $\mathbf{x}$ falsy. To describe how erroneous $\mathbf{x}$ 
measurement results in false $\mathbf{y}$ positions, 
we start formulazing it with Bayes rule:

\begin{equation}
  p(\mathbf{x}_0|\mathbf{y}_0) = \frac{p(\mathbf{x}_0) \cdot p(\mathbf{y}_0|\mathbf{x}_0)}{p(\mathbf{y}_0)}
\end{equation}

where $p(\mathbf{y}_0) = \int p(\mathbf{x}_0) \cdot p(\mathbf{y}_0|\mathbf{x}_0) d\mathbf{x}_0$
is independent of $\mathbf{x}_0$. Assuming that we don't have any previous 
knowledge where $\mathbf{x}$ is, the image plane is infinitely 
large, and both $p(\mathbf{x}_0)$ and $p(\mathbf{y}_0)$ are uniform, we can transform 
the conditional probability to pure inversion of conditionings:

\begin{equation}
  p(\mathbf{x}_0|\mathbf{y}_0) = p(\mathbf{y}_0|\mathbf{x}_0)
\end{equation}

In real-world, we measure projected points which is $\mathbf{y}$ and then 
back-project these measures to $\mathbf{x}$. Therefore, we are interested in 
knowing $p(\mathbf{x}_0|\mathbf{y}_0)$. For doing so, we make a further 
assumption that the measurement noise is Gaussian. Note that, to generalize our 
problem to all time states, we drop $k=0$ and compute for $p(\mathbf{x}|y)$. 
First, to remind the projection functions from $\mathbf{x}=(X_{cam},Z_{cam}) \in \R^2$ to 
$\mathbf{u}=U \in \R^1$, we write the following equation by assuming that 
images pixel coordinates are already back-distorted and 
intrinsic and extrinsic parameters of the RGB camera is known after calibration
(see notation \ref{eq:proj_func_w_square_pix_skew}):

\begin{equation}
  U = f_x \frac{X_{cam}}{Z_{cam}} + c_x + \epsilon
\end{equation}

where $\epsilon$ is Gaussian additive noise. Considering what we measure 
$y=U+\epsilon$, we can write the probability density functions
to represent the uncertainty since we have inversion of conditioning property:

\begin{equation}
  p(\mathbf{x}|y) = \mathcal{N}(y-f_x \frac{X_{cam}}{Z_{cam}} + c_x, \sigma_u^2) \text{ or }
  p(y|\mathbf{x}) = \mathcal{N}(f_x \frac{X_{cam}}{Z_{cam}} + c_x-y, \sigma_u^2)
\end{equation}

Notice that the back-projection axis would not pass 
through the real $\mathbf{x}$ position because of error in features. 
However, if we estimate the Gaussian 
parameters sufficiently, we will have a strong idea where the measured
point lies on XZ axis in the worst case. In statistics, one generally represent the 
worst case scenarios with $3\sigma$ boundaries that covers  99.7\% of probability
for one-dimensional Gaussian distribution as the following figure shows:

\begin{figure}[H]
	\centering
  \includegraphics[width=0.7\linewidth,natwidth=640,natheight=640]
  {fig/ref_imgs/conic_ray_2d_model_stds.png}
  \caption{Conic Ray 2D Error Model with Different $\sigma$ Values}
	\label{fig:conic_ray_2d_error_model_stds}
\end{figure}

So far, we only consider projections from 2D points in world onto 1D plane. 
However, extension of this model into a model where 3D points in world are 
projected onto 2D plane is a trivial operation: 

\begin{figure}[H]
	\centering
  \includegraphics[width=0.7\linewidth,natwidth=640,natheight=640]
  {fig/ref_imgs/conic_ray_3d_model.png}
  \caption{Conic Ray 3D Error Model with $3\sigma$ Values}
	\label{fig:conic_ray_3d_error_model}
\end{figure}

The rays that emanate from optical centre through the elliptic region form 
uncertainty region (represented with the dashed lines) 
based on distance of the measured point cloud to the image plane. This 
corresponds to the conic ray model we discussed earlier. To formulate this model, we need to 
find parameters ellipse (more specifially called confidence ellipses) 
based the noise for each axis on image plane.
To do so, we utilize the multi-dimensional Gaussian distributions under the 
same assumption of our simplified case:

\begin{equation}
  p(\mathbf{x}|\mathbf{y}) = \mathcal{N}(\mathbf{y}-\mathbf{x, \mathbf{Q}}) = 
  \frac{1}{\sqrt{(2\pi)^n|\mathbf{Q}|}} 
  \exp(-\frac{1}{2} (\mathbf{y}-\mathbf{x})^\intercal \mathbf{Q}^{-1} (\mathbf{y}-\mathbf{x}))
\end{equation} \label{eq:cov_ellipse}

where $n=2$ for a confidence ellipse,

\begin{equation}
  \mathbf{y} = \begin{bmatrix}u \\ v \end{bmatrix} \text{, } 
  \mathbf{x} = \begin{bmatrix} f_x \frac{X_{cam}}{Z_{cam}} + c_x \\ f_y \frac{Y_{cam}}{Z_{cam}} + c_y \end{bmatrix} \text{,and }
  \mathbf{Q} = \begin{bmatrix} \sigma_u^2 & 0 \\ 0 & \sigma_v^2 \end{bmatrix}
\end{equation}

On the other hand, the conic ray model still misses one more component; 
that is, the depth noise which causes an uncertainty 
along the axis that is perpendicular to image plane. 
This is, however, caused by a IR depth camera in Kinect. 
In the following section, we will discuss about how to extend the current conic ray 
model with depth uncertainty.

\subsection{Depth Related Uncertainties} \label{sb_sc_depth_uncertainty}

Modeling noise in depth measurements is more complicated than RGB camera. We 
already discussed how structured IR light speckles are projected onto an object 
so that IR camera can capture its disformed patterns. During this process, 
many things can go wrong. For example, (1) certain ambient background would make 
Kinect suffer from over-saturated disparity image, (2) having multiple Kinect 
in the same environment can lead to interference issue, (3) multi-path 
propagation of the light might change the expected illumination, or (4) measuring 
in dynamic scene might result in improper IR light patterns. All of these 
non-deterministic events makes it harder to model the uncertainty. However, 
we assume that operating conditions and evironment are chosen carefully 
in order to avoid these event as much as possible. What we aim to model in this 
section is mostly systematic errors in Kinect. For doing so, we rely on 
the experiments that is done by THIS GUY \cite{}. 

\subsubsection{Kinect's Systematic Depth Noise}

An experimental analysis is conducted to measure Kinect's depth noise by 
THIS GUY \cite{}. To do so, 
they calculated the difference between ground truth and Kinect measurements. 
They found out that there are two types of systematic noise occuring in Kinect's depth 
measurements: \textit{axial} noise and \textit{lateral} noise.


\begin{figure}[H]
	\centering
  \includegraphics[width=0.7\linewidth,natwidth=640,natheight=640]
  {fig/ref_imgs/kinect_noise_model.png}
  \caption{Depth Noise}
	\label{fig:kinect_noise_model}
\end{figure}

To detect the lateral and axial noise, they built an experimental setup with a 
Kinect that projects its IR speckle patterns onto a planar surface 
(see figure \ref{fig:kinect_noise_model}). Then, they collect 
depth measurements at different distance to the planar surface 
positioning at different angles. 
For calculating the axial noise, they (1) remove the lateral noise cropping 
edges, (2) then remaining depth region is fitted a plane that has minimum 
error to the ground truth and (3) they finally calculated 
distance difference between measured depth and ground truth.
On the other hand, 
the lateral noise is simply calculated by taking pixel difference between 
fitted straight edge passing through center of distribution and 
measured (zigzag-like shape) pixels.


\begin{figure}[H]
	\centering
  \includegraphics[width=0.7\linewidth,natwidth=640,natheight=640]
  {fig/ref_imgs/kinect_noise_experiment.png}
  \caption{Kinect Depth Noise Experiment}
	\label{fig:kinect_noise_experiment}
\end{figure}

After calculating regarding errors between measurements and ground truth, 
they realized that the axial and lateral noise have different noise 
characteristics. The lateral noise error distribution stays constant with 
the distance as seen in figure \ref{fig:kinect_noise_hist}. Whereas, 
the axial noise distribution gets wider with the 
increased distance.


\begin{figure}[H]
	\centering
  \includegraphics[width=0.7\linewidth,natwidth=640,natheight=640]
  {fig/ref_imgs/kinect_noise_hist.png}
  \caption{Kinect Depth Error Histogram at Different Distances}
	\label{fig:kinect_noise_hist}
\end{figure}

However, the axial noise has another property, which is the response to the 
different angles. Notice the following figure that shows this issue. The
standard deviation of the axial noise are increased drastically after 
60 degrees. 


\begin{figure}[H]
	\centering
  \includegraphics[width=0.7\linewidth,natwidth=640,natheight=640]
  {fig/ref_imgs/kinect_noise_fitted_model.png}
  \caption{Kinect Noise Fitted Model}
	\label{fig:kinect_noise_fitted_model}
\end{figure}

In the light of these experiments, they proposed two empirical 
models that fit corresponding measurements for the axial and lateral noise.
For axial noise, they define a 
quadratic relationship between standard deviation of the error and distance 
along z-axis. 

\begin{equation}
  \sigma_z (z,\theta) = 0.0012 + 0.0019 \cdot (z-0.4)^2 \text{, }
  \ang{10}\leq \theta \leq \ang{60}
\end{equation}

where $z$ is the measured depth metric. 
Plus, they added a hyperbolic parameter to represent the 
behavior measurement error beyond 60 degrees:

\begin{equation}
  \sigma_z (z,\theta) = 0.0012 + 0.0019 \cdot (z-0.4)^2 + 
  \frac{0.0001}{\sqrt{z}} + 
  \frac{\theta^2}{(\pi/2 - \theta)^2}
  \text{, }
  \theta \geq \ang{60}
\end{equation} \label{eq:axial_noise_w_hyperbolic}

For lateral noise, remember that its noise was almost constant with respect to 
distance along z-axis and had the similar hyperbolic effect after 60 degrees of 
angle. Hence, they defined lateral noise with the following equations:

\begin{equation}
  \sigma_L(\theta) = 0.8 + 0.035 \cdot \theta/(\pi/2-\theta) \text{ (in pixels)}
\end{equation}

This is how Kinect's depth noise is characterized experimentally by 
THIS GUY \cite{}. To validate these noise models' correctness, they implemented a 3D 
reconstruction and a camera pose tracking scenario and the noise models 
improved the overall accuracy of both application. It is important to 
note that they cooperated the iterative closest point (ICP) to estimate 
the camera poses. The ICP is one of many algorithms to solve VO problem. 
With the ICP, one utilizes all or most of the 3D point clouds instead of 
selecting distinct feature in each frame. Therefore, we need to find a way 
to integrate the depth noise model into our feature-based VO pipeline.

\subsubsection{How To Update the Conic Ray Model with Depth Noise?}

The missing component of our conic ray model is depth uncertainty since 
we only defined confidence ellipses caused by feature related uncertainties so far.
We mean the depth uncertainty that causes errors along z-axis direction. 
This infact refers to the axial noise, which is one of Kinect's noise models 
we discussed earlier in this section.
Thus, we can now extend our conic ray model with the axial noise 
(see notation \ref{eq:axial_noise_w_hyperbolic}).

\begin{figure}[H]
	\centering
  \includegraphics[width=0.7\linewidth,natwidth=640,natheight=640]
  {fig/ref_imgs/conic_ray_3d_model.png}
  \caption{Conic Ray 3D Error Model with Depth Uncertainty}
	\label{fig:conic_ray_3d_error_model}
\end{figure}

\begin{figure}[H]
	\centering
  \includegraphics[width=0.3\linewidth,natwidth=640,natheight=640]
  {fig/ref_imgs/ellipsoid.png}
  \caption{REPLACE CONFIDENCE ELLIPSE WITH ELLIPSOID}
	\label{fig:conic_ray_3d_error_model}
\end{figure}

Then, let's update our confidence ellipse (see \ref{eq:cov_ellipse}) formulation 
to confidece ellipsoid so that we can represent the uncertainty of a point cloud 
in 3D world.

\begin{equation}
  p(\mathbf{x}|\mathbf{y}) = \mathcal{N}(\mathbf{y}-\mathbf{x, \mathbf{Q}}) = 
  \frac{1}{\sqrt{(2\pi)^n|\mathbf{Q}|}} 
  \exp(-\frac{1}{2} (\mathbf{y}-\mathbf{x})^\intercal \mathbf{Q}^{-1} (\mathbf{y}-\mathbf{x}))
\end{equation}

where $n=3$ now for a confidence ellipsoid,

ADD i th INDECES. for ex. $u_i$. VERIFY THE EQUATION

\begin{equation}
  \mathbf{y} = \begin{bmatrix} u_i \\ v_i \\ z_i \end{bmatrix} \text{, } 
  \mathbf{x} = 
  \begin{bmatrix} 
    f_x \frac{X_{cam}}{Z_{cam}} + c_x \\ 
    f_y \frac{Y_{cam}}{Z_{cam}} + c_y \\
    \frac{1}{(\frac{m}{fb})d' + (Z_r^{-1} + \frac{n}{fb})}
  \end{bmatrix} \text{,and }
  \mathbf{Q} = 
  \begin{bmatrix} 
    \sigma_u^2 & 0 & 0 \\ 
    0 & \sigma_v^2 & 0 \\
    0 & 0 & \sigma_z(z, \theta)^2
  \end{bmatrix}
\end{equation}

HOW ABOUT ANGLE AND HYPERBOLIC?

As seen, the axial noise can be embedded as the third dimension along the z-axis. 
The lateral noise, on the other hand, has an indirect relationship to overall 
uncertainty of a point cloud. This indirect relationship occurs when associating 
depth pixel coordinates with the color pixel coordinates. Therefore, one can 
avoid this data association error by applying a smoothing filter on depth images. 
Also, remember that lateral noise of the Kinect was mostly around one pixel. 
Thus, a 3x3 smoothing filter can be used on features or edges to compensate the error.

\section{Weighted Least-Squares Optimization} \label{sc_weighted_lsq}

\section{Covariance Estimation} \label{sc_covariance_estim}

% ***************************CP5-EVALUATION***************************
\chapter{Evaluation} \label{cp_evaluation}

\subsubsection{Ideal Case} \label{sb_sb_sc_ideal_case}

\subsubsection{Outliers in Feature Matches} \label{sb_sb_sc_outliers}


% ***************************CP6-CONCLUSION***************************
\chapter{Conclusion} \label{cp_conc}

% ***************************CP7-EVALUATION***************************
\chapter{Reference} \label{cp_ref}


% ***************************CP7-EVALUATION***************************
\chapter{Appendices} \label{cp_appendices}

\section{Rigid-Body Transformations} \label{sc_rigid_body_transformations}

\begin{figure}[H]
	\centering
	\includegraphics[width=\linewidth,natwidth=640,natheight=640]
	{fig/ref_imgs/2d_local_coords_in_world_coords.png}
  \caption{A Local Coordinate System in World Coordinate System}
	\label{fig:2d_local_coords_in_world_coords}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=\linewidth,natwidth=640,natheight=640]
	{fig/ref_imgs/2d_rotation.png}
  \caption{Rotating the Local Coordinate System in World Coordinate System}
	\label{fig:2d_rotation}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=\linewidth,natwidth=640,natheight=640]
	{fig/ref_imgs/2d_translation.png}
  \caption{Translating the Local Coordinate System in World Coordinate System}
	\label{fig:2d_rotation}
\end{figure}




\subsection{3D Rotation with Quaternions}


\begin{figure}[H]
	\centering
	\includegraphics[width=\linewidth,natwidth=640,natheight=640]
	{fig/ref_imgs/2d_translation.png}
  \caption{Translating the Local Coordinate System in World Coordinate System}
	\label{fig:2d_rotation}
\end{figure}



\subsection{3D Translation}


\subsection{3D Transformation}

\section{Least Squares}\label{sc_least_squares}

Throughout this thesis, least squares method empowered many different components 
of out VO system, 
such as camera calibration, RANSAC and most importantly motion estimation, 
Therefore, we will discuss underlying principles of least squares method in this section.

Ultimately, error minimization is an 
operation which wish to get the maximum likelihood of the function. To do so,
we search the most
likely state configuration as close as possible to its exact and ideal solution. 
In the case of any optimization problems,
the goal is to find interesting points, such as local/global
maximum or local/global minimum, on the \textit{objective}
\textit{function}. However, due to the
non-linearity and noise in measurements, one can only approximate a solution.
One way to solve such problems is to generate a quadratic model of 
the objective function and iterate
through the function using \textit{Newton's methods}. 
For example,
an optimal solution (or an interest point)
Figure-\ref{fig:lsq_multivariable_function_example} is at the local
minimum of the function that is highlighted as a red point cloud.

\begin{figure}[H]
	\centering
	\includegraphics[width=\linewidth,natwidth=640,natheight=640]
	{fig/lsq_multivariable_function_example.jpg}
	\caption{Local Minimum at a Quadratic Function}
	\label{fig:lsq_multivariable_function_example}
\end{figure}


In optimization literature,
there are many versions of Newton's method 
and they all try to find the
local maximum/minimum in the most efficient and accurate way.
However, in the end, they all solve the problem
with the \textit{gradient descent} manner.
%the approximated Hessian function, is one of them and we will be using
%it in our slam algorithm.
To elaborate the problem, we provide a regression example 
which can be infact solved with linear least squares techniques but it can serve 
as a simple toy example throughout our explainations.

Suppose that we have a model function $g(x;a)$. However,
we don't know what the $x=(x_1,x_2)$ coefficients (so-called
\textit{optimization} \textit{parameters}) are and we can only
plug $a$, which is the \textit{independent} variable, into the
\textit{model} $S$ to see
how the output of the model changes given the independent variable. 

\begin{figure}[H]
	\includegraphics[width=0.8\linewidth,natwidth=640,natheight=640]
	{fig/lsq_model.jpg}
	\centering
	\caption{Least Square Model}
	\label{fig:lsq_model}
\end{figure}



\begin{gather}
\xi = a \text{ (independent variable)} \in \R \\
\eta = S \text{ (dependent variable)} \in \R
\label{eq:lsq_model_variables}
\end{gather}

\begin{equation}
S = g(x;a) := x_1 + x_2a \quad 
\text{where} \quad 
x=(x_1,x_2) \in \R^2
\label{eq}
\end{equation}


To see this effect, 
we draw a graph that is shown in
Figure-\ref{fig:lsq_curve_fit_measurements}.

\begin{figure}[H]
	\centering
	\includegraphics[width=\linewidth,natwidth=640,natheight=640]
{fig/lsq_curve_fit_measurements_v2.jpg}
	\caption{Least Squares Measurements}
	\label{fig:lsq_curve_fit_measurements}
\end{figure}


Our goal is now to find a function, which will fit this
dataset. This is a typical least squares curve fitting problem.
In this problem, we construct a \textit{residuals function} $r_i(x)$ by providing
error values between model estimation $g(x;a_i)$ and dependent
variable $S_i$. In this case, the dependent variable 
represent the real world
measurements and the residuals function represents the error between the estimated value and measurement value. 

\begin{equation}
r_i(x) := g(x;a_i) - S_i \qquad \text{ for }  i = 1,\dots,m.
\label{eq:}
\end{equation}

The residuals function is usually squared to magnify larger error effect:

\begin{equation}
\begin{aligned}
F(x) & := \sum_{i=1}^{m} \vert r_i(x) \vert^2 = 
\sum_{i=1}^{m} \vert g(x;a_i) - S_i \vert^2 =
\sum_{i=1}^{m} (g(x;a_i) - S_i)^2 \\
\label{eq}
\end{aligned}
\end{equation}

Now, one can use the sum of squared error function to calculate the most likely
configuration that can minimize the errors. 

\begin{equation}
X^* = \argmin_x F(x) = 
\sum_{i=1}^{m} (g(x;a_i) - S_i)^2, 
\quad x \in \R^2
\label{eq}
\end{equation}

At this point, the objective function is ready to be handed over to 
to any gradient decent based least squares solver. The method will try to find the
\textit{optimal} solution $X^*$ by minimizing the objective function.

\begin{equation}
\text{Minimize} \quad \sum_{i=1}^{m} (g(x;a_i) - S_i)^2, 
\quad x \in \R^2
\label{eq}
\end{equation}

To help our understanding, the objective function is drawn in
Figure-\ref{fig:lsq_sum_of_squared_error_function}.
As we can see, the based on the given $x=(x_1,x_2)$ values, 
the objective
function $F(x)$ behave as follows:

\begin{figure}[H]
	\centering
	\includegraphics[width=\linewidth,natwidth=640,natheight=640]
	{fig/lsq_sum_of_squared_error_function_v2.jpg}
	\caption{Local Minimum at Sum of Squared Error Function}
	\label{fig:lsq_sum_of_squared_error_function}
\end{figure}

The initial guess for the optimization parameters at $x=(4,4)$, which
is highlighted as a blue color point cloud and 
the local minimum point, which is the interest point of ours, 
is highlighted with the red cloud. What the gradient descent method essentially 
does is to travel from the initial guess point to the nearest local minimum 
on the objective function. As we can see
that the descent is successfully performed and the optimization
operation results at $x=(1.9,2.1)$. 


If we now place the optimization variables into our model function, we
get the $g(x;a)=1.9+2.1a$ and a fitted line based on the given
dataset in
Figure-\ref{fig:lsq_curve_fit_operation}.

\begin{figure}[H]
	\centering
	\includegraphics[width=\linewidth,natwidth=640,natheight=640]
	{fig/lsq_curve_fit_operation_v2.jpg}
	\caption{Least Squares Curve Fitting Operation}
	\label{fig:lsq_curve_fit_operation}
\end{figure}

Now that we have an intiution how least squares problems are solved 
by gradient descent methods, let's further discuss one of them.

\subsection{Gradient Descent}

In order to understand non-linear least square algorithms, 
we first need to look at how a gradient descent algorithm works. For instance; 
assuming that the objective function $F(x)$ is differentiable and its 
derivatives has the following form:

\begin{equation}
  \nabla F(x) = F'(x) = \frac{\partial F(x)}{\partial x}
\end{equation}

\begin{figure}[H]
	\centering
	\includegraphics[width=\linewidth,natwidth=640,natheight=640]
	{fig/ref_imgs/taylor_1st_derivative.png}
	\caption{First-order Derivative of Taylor Expansion}
  \label{fig:taylor_1st_derivative}
\end{figure}



\begin{equation}
  \nabla^2 F(x) = F''(x) = \frac{\partial^2 F(x)}{\partial x^2}
\end{equation}

\begin{figure}[H]
	\centering
	\includegraphics[width=\linewidth,natwidth=640,natheight=640]
	{fig/ref_imgs/taylor_2nd_derivative.png}
	\caption{Second-order Derivative of Taylor Expansion}
  \label{fig:taylor_2nd_derivative}
\end{figure}


Another fact about $F(x)$, considering above formula, is that its value will decrease 
in the direction of its negative derivative:

\begin{equation}
  \Delta x = - 
  \frac{\partial^2 F(x)}{\partial x^2}^{-1}
  \frac{\partial F(x)}{\partial x}
\end{equation}\label{eq:delta_x_descent_direction}

That being in mind, one can 
claim that if we iteratively travel through the function in the direction of its negative 
derivative, we would eventually reach to a nearest local minima with respect to the starting 
point. This is the fundamental idea behind gradient descent algorithm.
On the other hand, using its derivative directly does not ensure that 
we would reach to any local minima in some cases. Thus, a tuning factor so-called 
\textit{step length} $\alpha$ is used to adjust the size of length:

\begin{equation}
  \Delta x = -\alpha 
  \frac{\partial^2 F(x)}{\partial x^2}^{-1}
  \frac{\partial F(x)}{\partial x}
\end{equation}\label{eq:delta_x_step_length}


Assuming that we have an educated initial guess $x_0$ from which we start to search 
for a local minimum, we can describe the gradient descent algorithm as follows:

\begin{enumerate}
  \item take the derivative of $F(x)$ at the current $x_k$
  \item find the sufficient step length $\alpha$
  \item iterate $x_{k+1} := x_k - \alpha \frac{\partial F(x_k)}{\partial x}$ until it converges
\end{enumerate}

As we see, the gradient descent is a simple algorithm but it is an iterative 
approach that might require an exhaustive search operation if the initial 
guess is not given intellegently. Therefore, it can get very complicated 
if convergence speed and convergence success is considered. For this reason, 
there are many version of the algoritm that aims to solve in the most efficient 
manner.

\subsection{Levenberg-Marquardt}
Levenberg-Marquardt (LM) is one of most popular gradient descent based algoritm 
to solve least squares problem. In this section, we describe the idea behind 
the LM algorithm. Assume that we have a $r(x)$ residuals function:

\begin{equation}
  r(x) = \begin{pmatrix} r_1(x) \\ \vdots \\ r_m(x) \end{pmatrix} \in \R^m
\end{equation}

To find a local maximum/minimum of the residuals function, we need to 
determine the first derivative and it 
$r'(x)$ appears in a special form, e.g., Jacobian:

\begin{equation}
  J(x) = \begin{bmatrix} \frac{\partial}{\partial x_j }r_i(x) \end{bmatrix}_{ij} 
  = 
  \begin{bmatrix} 
    \vertbar & & \vertbar \\
    \frac{\partial}{\partial x_1} & \dots & \frac{\partial}{\partial x_1} \\
    \vertbar & & \vertbar
  \end{bmatrix}
  = 
  \begin{bmatrix}
    \horzbar & \nabla r_1(x)^T & \horzbar \\
     & \vdots & \\
    \horzbar & \nabla r_m(x)^T & \horzbar 
  \end{bmatrix}
  \in \R^{mxn}
\end{equation}

Least squares problem has special forms which can be exploitted. 
Here, the residuals function function and its derivatives is given:
%(Keep in mind that multiplication with $\frac{1}{2}$ in front of objective function
%is for cosmetic reason since it does not effect local mimimum's position in the function)

\begin{equation}
  F(x) = \frac{1}{2} ||r(x)||^2 = \frac{1}{2} r(x)^T r(x) \text{  (Objective function)}
\end{equation}\label{eq:residuals_objective}
\begin{equation}
  \nabla F(x) = J(x)^T r(x) = \sum_{i=1}^{m} r_i(x) \nabla r_i(x) \text{  (First-order derivative)}
\end{equation}\label{eq:residuals_objective_first_der}
\begin{equation}
\nabla^2 F(x) = J(x)^TJ(x) + \sum_{i=1}^m r_i(x) \nabla^2 r_i(x) \text{ (Second-order derivative)}
\end{equation}\label{eq:residuals_objective_second_der}

NOTE: Move this part to gradient descent ->
The objective function is usually assumed to be highly non-linear; therefore, 
one cannot perform computation such as derivatives. Therefore, we build 
a $q_{LM}$ quadratic model of the objective function using Taylor expansion:

\begin{equation}
  q_{LM}^k(d) = f(x^k) + \nabla f(x^k)^T d + \frac{1}{2} d^T B_{LM}^kd
\end{equation}

Given \ref{eq:residuals_objective} and \ref{eq:residuals_objective_first_der}, we elaborate the quadratic model:

\begin{equation}
  q_{LM}^k(d) = \frac{1}{2}r(x^k)^Tr(x^k) + r(x^k)^TJ(x^k)d + \frac{1}{2}d^TB_{LM}^kd
\end{equation}

It is critical to note that LM does not use $\nabla^2F(x)$ 
the exact second-order derivative (\ref{eq:residuals_objective_second_der})
(also appears in the special form called \textit{Hessian}) in its quadratic model. 
This is because it is computationaly expensive. Instead, we use an approximated 
Hessian model by removing $\sum_{i=1}^mr_i(x)\nabla^2r_i(x)$ and replacing with 
term $\lambda^k I$. Finally, the final approximated Hessian function would be:

\begin{equation}
  B_{LM}^k = J(x^k)^T J(x^k) + \lambda^k I
\end{equation}

where $\lambda^k>0$ is a positive number and $I\in \R^{nxn}$ is the identity matrix.

\begin{figure}[H]
	\centering
	\includegraphics[width=\linewidth,natwidth=640,natheight=640]
	{fig/ref_imgs/taylor_approx_2nd_derivative.png}
	\caption{Approximated Second-order Derivative of Taylor Expansion}
  \label{fig:taylor_approx_2nd_derivative}
\end{figure}




Similar to \ref{eq:delta_x_descent_direction}, one can calculate the descent direction 
in the context of LM method as follows:

\begin{equation}
  B_{LM}^kd^k = - \nabla f(x^k) \text{ or } d^k = - (B_{LM}^k)^{-1}\nabla f(x^k)
\end{equation}\label{eq:lm_descent_direction}

If we can write above equation using the content of Hessian model more explicitly: 

\begin{equation}
  [J(x^k)^TJ(x^k) + \lambda^kI]d^k = -J(x^k)^Tr(x^k)
\end{equation}\label{eq:damping_full}

it gives a clearer picture why $\lambda^kI$ term is used. In other gradient-descent 
based algorithm performs line search to determine the step length of the current 
iteration. In LM, this is done by tuning $\lambda$ parameter, also known as 
\textit{damping parameter}. For example, suppose that we assign $\lambda$ 
a significantly small value. Then, \ref{eq:damping_full} becomes:

\begin{equation}
  \lambda^kd^k \approx -J(x^k)^Tr(x^k) \text{ or } 
  d^k \approx -\frac{1}{\lambda^k}J(x^k)^Tr(x^k)\text{ or } 
  d^k \approx -\frac{1}{\lambda^k}\nabla F(x^k)
\end{equation}

Whereas, if we assign $\lambda$ a significantly large value, then it becomes:

\begin{equation}
  J(x^k)^TJ(x^k)d^k = -J(x^k)^Tr(x^k)
\end{equation}

which is a regular \textit{Newton step} (corresponds to $\alpha=1$ in \ref{eq:delta_x_step_length}).

On the other hand, another question arises about damping parameter about how to tune the parameter 
so that it will allow the algorithm to converge to a local minimum efficiently 
and accurately. This is done by the \textit{progress ratio} test:

\begin{equation}
  \rho^k = \frac{F(x^k) - F(x^k+d^k)}{q_{LM}^k(0)-q_{LM}^k(d^k)} =
  \frac{\text{actual decrease in objective } F(x)}
  {\text{predicted decrease by model } q_{LM}^k(d)}
\end{equation}\label{eq:lm_progress_ration}

Based on the $p^k$, we can create an empiric strategy:

\begin{enumerate}
  \item If $p^k \geq t_2$, then it is considered as a very successful step; 
    therefore, we can even choose smaller value for damping facor in the next iteration 
    so that 
    we increase the descent speed.
  \item If $t_1 \leq p^k < t_2$, then it is still a successfull step but we 
    can keep the damping factor same in the next iteration so that we don't 
    miss out the local minimum.
  \item If $p^k < t_1$, then it is a bad step; therefore, we can reject this 
    damping factor choice and choose a larger value.
\end{enumerate}

Fundemantally, this is how LM algoritm works. 
One must keep in mind that even the sophisticated LM algorithm might fail to 
converge a desired interest point on the objective function.
There are two crucial factors on which any gradient descent based algorithm depends:
\begin{itemize}
  \item \textit{outliers} in measurement dataset,
  \item good \textit{initial guess}. 
\end{itemize}

It is important that we provide a good initial
guess and remove outliers from dataset. 
If these two criteria do not meet, LM might converge to the
another local minimum or might not even converge to
an optimal solution. 
That being said, we can now summarize the algorithm into five steps:

\begin{enumerate}
  \item build the quadratic model $q_{LM}^k(d)$ of the objective function,
  \item compute the descent direction $d^k$ by solving the linear system of 
    equations in \ref{eq:lm_descent_direction},
  \item calculate the progress ratio $\rho^k$ in \ref{eq:lm_progress_ration}.
  \item choose the next damping factor $\lambda^{k+1}$ according to progress ratio test,
  \item set the next iteration based on progress ratio test:
    $\\ \text{  if } \rho^k > t_1 \rightarrow x^{k+1}:=x^k +d^k \text{ (step accepted) }\\ 
    \text{  if }\rho^k > t_1 \rightarrow x^{k+1}:=x^k \text{ (step rejected)}$
\end{enumerate}



\section{Error Propagation Law}

\begin{figure}[H]
	\centering
	\includegraphics[width=\linewidth,natwidth=640,natheight=640]
	{fig/ref_imgs/error_propagation.png}
	\caption{Error Propagation}
  \label{fig:error_propagation}
\end{figure}



\end{document}


